{"componentChunkName":"component---src-templates-blog-post-js","path":"/ai-이노베이션-스퀘어-7일차/","result":{"data":{"markdownRemark":{"html":"<h3 id=\"7일차\" style=\"position:relative;\">7일차<a href=\"#7%EC%9D%BC%EC%B0%A8\" aria-label=\"7일차 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h3>\n<p>선형 회귀. 오랜만이다.</p>\n<h4 id=\"메모\" style=\"position:relative;\">메모<a href=\"#%EB%A9%94%EB%AA%A8\" aria-label=\"메모 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h4>\n<ul>\n<li>수치미분 최종버전 ★</li>\n<li>chain rule insight\n문제 풀이 시간에 배운 점(리마인드)</li>\n</ul>\n<hr>\n<h3 id=\"1-linear-regression\" style=\"position:relative;\">1. Linear Regression<a href=\"#1-linear-regression\" aria-label=\"1 linear regression permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h3>\n<p>(1) 학습 Learning 의 개념</p>\n<p><strong>\"how to find equation\"</strong></p>\n<p>x1과 t1의 관계를 알아내는 게 학습 목적이다. 이를 위해 SVN 등을 쓴다.</p>\n<p>입력x 가지고 ML을 통해서 출력y 나오는데, 이 계산 결과y와 정답 t 둘이 거의 같아지면?</p>\n<p>입력을 주고 정답 t와 비슷해지게 y 수식을 만들면 어느 값이 들어와도 정답t이랑 비슷한 값을 찾아낼 수 있다. 이 y 수식의 형태를 찾는 게 학습이다.</p>\n<p>입력 x로 만든 계산 결과y가 정답t이랑 비슷하다는 것은 정답과 계산값이 오차가 작다는 의미이다.</p>\n<p>​<strong>\"ML is how to find equation in order to minimize between target and calculated value\"</strong></p>\n<p>입력(x)과 출력(y) 은 다음의 형태로 나타낼 수 있다.</p>\n<p>$y = W*x + b$</p>\n<p>=> x는 정해져 있으니 W와 b가 변하면 y가 변한다<br>\n=> 입력값에 변화를 만들어내고 (x<em>W) 평균을 취해서 (+b) 계산한\n값 (W</em>x+b)이 정답 t과 차이가 작아지도록 W,b를 구하는 게 학습의 개념이다</p>\n<p>​(2) 오차 Error</p>\n<p>ML에서는 오차 t-y=t-(wx+b) 들의 합이 최소가 되게 하는 W, b 찾아야 한다.\n​\n오차를 단순히 더하면 0이 될 수 있어 각 오차를 제곱 (t-y)^2 하여 평균 낸다. 그래서 오차는 항상 양수이다.</p>\n<p>cf. 양수를 만드는 방법에는 절대값 / 제곱이 있는데, ML에서는 제곱을 취한다. (* 복소수 배제함)  </p>\n<ul>\n<li>오차에서 제곱을 쓰는 이유, insight<br>\n제곱을 하는 경우, 오차 값이 작은 건 더 작게 보여주고, 오차가 크면 더 (커서) 많이 반응해서이다.</li>\n</ul>\n<p>error = loss = [t - (W*x + b) ] ^2 => W, b의 함수</p>\n<p>W가 변한다고 b가 100% 변하는 게 아니므로, W, b는 독립 관계이다. t, x는 상수 값이다.</p>\n<p>=> <strong>loss = E(W,b) = [ t-(W*x+b) ]^2</strong></p>\n<p>손실 함수 E(W,b) 가 최소값을 갖도록 (W, b) 를 구하는 것이 (linear) regression model 의 최종 목적이다.</p>\n<p>​cf. 평균제곱오차 MSE ( Mean Square Error ) made by 가우스</p>\n<p>$y = (Σ [t_i-(x*W+b)]^2 )/ n$</p>\n<p>(3) 가중치 insight  </p>\n<p>가중치 W는 정답 target 만드는 입력 input 갯수만큼 필요하다. 그 데이터에 변화를 줘야하기 때문이다. 입력data 각각의 변화를 위해 행렬 곱 연산을 사용한다.</p>\n<p>=> $y = X*W + b$</p>\n<p>ex) input (1,2,3) -> target (0)<br>\ntarget 0 하나를 위해 input 3개 필요하니, 가중치도 3개 필요하다  </p>\n<p>입력 각각은 독립적이니까 $y = w1*x1 + w2*x2 + w3*x3 + b$  이렇게 표현할 수 있다.</p>\n<p>ex) input (1) -> target (2) : w 1개 필요</p>\n<p>cf. The Gradients of specified image is that, Y = a GramMatrix avg / a image</p>\n<ul>\n<li>gradient = 미분 활용</li>\n</ul>\n<hr>\n<h3 id=\"2-경사하강법-gradient-decent-algorithm\" style=\"position:relative;\">2. 경사하강법 gradient decent algorithm<a href=\"#2-%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95-gradient-decent-algorithm\" aria-label=\"2 경사하강법 gradient decent algorithm permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h3>\n<p>​(1) 손실 함수 Loss Function</p>\n<p>손실 함수의 계산</p>\n<p>손실함수 : $E(W,b) = (Σ [t_i-(W*x+b)]^2 )/ n$</p>\n<p>=> 하지만 오차 함수가 n차가 되면 함수를 그리기가 힘들어서 미분을 이용한다.<br>\n=> 기울기가 0이다 = 변화가 없다 = 최소값이다<br>\n=> gradient decent algorithm</p>\n<p>$W = W - (∂E(W,b) / ∂W) * a$<br>\n(a:학습율 Learning Rate)</p>\n<p>임의의 한 점에서 미분값을 구했을 때 양수이면 임의의 한 점이 감소시키는 방향으로, 아니면 그 반대인 증가시키는 방향으로 이동하게 된다.</p>\n<ul>\n<li>y=(x-a)^2+b 를 안 하는 이유? 2차함수는 이거는 입력변수 x가 하나이지만, y=w1x1+w2x2+b 이런 형태이면 그래프가 3차원으로 나온다. 여기까지가 한계이다.<br>\n=> 함수를 시각적으로 그릴 수 없어서 미분을 활용한다.</li>\n</ul>\n<p>​</p>\n<p>동일한 입력에 대해 w,b에 따라 y가 달라진다</p>\n<p>x에대해 y계산되고 e가 계산된다 최소인지는 모른다 반복해서 비교한다 w,b변하면 y2가 계산된다 ,,, 최소인지 아직 모른다,,, 또 반복한다,,,</p>\n<p>=> 반복하니까 for문 쓸 거고, 이 안에서 W,b 계속 바꿀 것임</p>\n<p>​</p>\n<ul>\n<li>ML = (x,t) 데이터정의하는 부분 / (임의의 w,b 값을 둬서) y 계산하는 부분, E 계산하는 부분 / w,b update하는 부분</li>\n</ul>\n<p>​</p>\n<p>데이터정의+손실함수정의+for문반복</p>\n<p>​</p>\n<p>xdata = np.array([1.0,2.0,3.0,4.0,5.0]).reshape(5,1) -> batch 한 번에 표시</p>\n<p>batch_size : 원래는 1개씩 넣어야하는데 5개 한번에 넣으니까 5</p>\n<p>​</p>\n<p>loss<em>func 함수임을 나타내고, loss</em>val 값을 나타내고 싶어서 굳이 나눴지 그냥 같은 것이다 ㅇㅇ</p>\n<p>학습 완료 후 (W,b setting 이후) predict</p>\n<p>​</p>\n<p>손실함수가 최소가 될 때까지 반복하며 W, b를 구함</p>\n<p>​</p>\n<p>cf. learning rate 값을 1e-2에서 1e-1로 바꿨다</p>\n<p>1e-6로 바꿈</p>\n<p>1e-2하고 8001을 2001로 바꿈</p>\n<p>​</p>\n<p>★ learning rate, epochs = 하이퍼파라미터</p>\n<p>경험,테스트로 알게될 값 => 값 찾는 게 어려움 ㅠ</p>\n<p>​</p>\n<p>​</p>\n<ol>\n<li>hyper parameter를 설정하고 2. 손실함수가 줄어드는 거 확인 (절대값 아니고 trend 경향!)</li>\n</ol>\n<p>​</p>\n<p>cf. divergence 발산할 수도 있다 => learning rate 감소시켜야 함</p>\n<p>​</p>\n<p>trade off 이율 배반</p>\n<p>속도랑 정확도 같이 얻기 힘들다 ㅠ</p>\n<p>=> 회사나 팀에서 결정함..</p>\n<p>​</p>\n<p>학습률 learning rate 얼마나 반복할 것인가 epochs</p>\n<p>​</p>\n<p>​</p>\n<p>하이퍼파라미터를 찾는 나만의 노하우...?</p>\n<p>​</p>\n<p>ex. 손실함수가 발산할 때, 수렴하도록하면 뭘 바꿔야할까 = learning rate</p>\n<p>​</p>\n<p>​</p>\n<p>정답을 만드는 입력 갯수3개니까 가중치 3개 필요</p>\n<p>​</p>\n<p>하이퍼파라미터 = 데이터에 민감, 환경에 민감</p>\n<p>=> 그러니 바꿔야 함</p>\n<p>​</p>\n<p>6이하로 안 내려가는 이유 : 데이터가 부족해서, 어디가 최소인지 코드 돌리기 전엔 몰라서</p>\n<p>​</p>\n<p>데이터는 다다익선, 백만 행 이상</p>\n<p>ML에서 어려운 건 하이퍼파라미터 세팅이다</p>\n<p>​</p>\n<p>local minimum global minimum</p>\n<p>local 벗어나려면,,, 다양한 파라미터 값을 써야함</p>\n<p>​</p>\n<p>np.random.seed(0) seed 값을 주면 특정 seed에 대해 W 값은 똑같이 나온다</p>\n<p>​</p>\n<p>ML 기본 패턴 - 데이터정의 (입력,정답) / W, b 정의 => y / 손실함수 구해서 for문으로 W, b update</p>\n<p>cf. AutoML</p>\n<p>​</p>\n<p>cf. 머신러닝 automl hyper parameter</p>","tableOfContents":"<ul>\n<li>\n<p><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-7%EC%9D%BC%EC%B0%A8/#7%EC%9D%BC%EC%B0%A8\">7일차</a></p>\n<ul>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-7%EC%9D%BC%EC%B0%A8/#%EB%A9%94%EB%AA%A8\">메모</a></li>\n</ul>\n</li>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-7%EC%9D%BC%EC%B0%A8/#1-linear-regression\">1. Linear Regression</a></li>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-7%EC%9D%BC%EC%B0%A8/#2-%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95-gradient-decent-algorithm\">2. 경사하강법 gradient decent algorithm</a></li>\n</ul>","frontmatter":{"title":"AI 이노베이션 스퀘어 12기 기본반 7일차 후기"}}},"pageContext":{"slug":"/ai-이노베이션-스퀘어-7일차/"}},"staticQueryHashes":["3159585216"]}