{"componentChunkName":"component---src-templates-blog-post-js","path":"/ai-이노베이션-스퀘어-7일차/","result":{"data":{"markdownRemark":{"html":"<h3 id=\"7일차\" style=\"position:relative;\">7일차<a href=\"#7%EC%9D%BC%EC%B0%A8\" aria-label=\"7일차 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h3>\n<p>선형 회귀. 오랜만이다. 이날 학습한 내용은 그 이전 동안 배웠던 파이썬 함수, numpy, 미분 등등 활용한 집약체의 느낌이 든다.</p>\n<h4 id=\"메모\" style=\"position:relative;\">메모<a href=\"#%EB%A9%94%EB%AA%A8\" aria-label=\"메모 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h4>\n<ul>\n<li>오차 제곱값 insight</li>\n<li>가중치 갯수 insight</li>\n<li>ML 기본 패턴</li>\n<li>Hyper Parameter</li>\n</ul>\n<hr>\n<h3 id=\"1-linear-regression\" style=\"position:relative;\">1. Linear Regression<a href=\"#1-linear-regression\" aria-label=\"1 linear regression permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h3>\n<p>(1) 학습 Learning 의 개념</p>\n<p><strong>\"how to find equation\"</strong></p>\n<p>x1과 t1의 관계를 알아내는 게 학습 목적이다. 이를 위해 SVN 등을 쓴다.</p>\n<p>입력x 가지고 ML을 통해서 출력y 나오는데, 이 계산 결과y와 정답 t 둘이 거의 같아지면?</p>\n<p>입력을 주고 정답 t와 비슷해지게 y 수식을 만들면 어느 값이 들어와도 정답t이랑 비슷한 값을 찾아낼 수 있다. 이 y 수식의 형태를 찾는 게 학습이다.</p>\n<p>입력 x로 만든 계산 결과y가 정답t이랑 비슷하다는 것은 정답과 계산값이 오차가 작다는 의미이다.</p>\n<p>​<strong>\"ML is how to find equation in order to minimize between target and calculated value\"</strong></p>\n<p>입력(x)과 출력(y) 은 다음의 형태로 나타낼 수 있다.</p>\n<p>$y = W*x + b$</p>\n<p>=> x는 정해져 있으니 W와 b가 변하면 y가 변한다<br>\n=> 입력값에 변화를 만들어내고 (x<em>W) 평균을 취해서 (+b) 계산한\n값 (W</em>x+b)이 정답 t과 차이가 작아지도록 W,b를 구하는 게 학습의 개념이다</p>\n<p>​(2) 오차 Error</p>\n<p>ML에서는 오차 t-y=t-(wx+b) 들의 합이 최소가 되게 하는 W, b 찾아야 한다.  ​\n오차를 단순히 더하면 0이 될 수 있어 각 오차를 제곱 (t-y)^2 하여 평균 낸다.<br>\n그래서 오차는 항상 양수이다.</p>\n<p>cf. (* 복소수 배제함) 양수를 만드는 방법에는 절대값 / 제곱이 있는데, ML에서는 제곱을 취한다.  </p>\n<ul>\n<li>오차에서 제곱을 쓰는 이유, insight<br>\n제곱을 하는 경우, 오차 값이 작은 건 더 작게 보여주고, 오차가 크면 더 (커서) 많이 반응해서이다.</li>\n</ul>\n<p>error = loss = [t - (W*x + b) ] ^2 => W, b에 대한 함수이다.</p>\n<p>W가 변한다고 b가 100% 변하는 게 아니므로, W, b는 독립 관계이다. t, x는 상수 값이다.</p>\n<p>=> <strong>loss = E(W,b) = [ t-(W*x+b) ]^2</strong></p>\n<p>손실 함수 E(W,b) 가 최소값을 갖도록 (W, b) 를 구하는 것이 (linear) regression model 의 최종 목적이다.</p>\n<p>​cf. 평균제곱오차 MSE ( Mean Square Error ) made by 가우스</p>\n<p>$y = (Σ [t_i-(x*W+b)]^2 )/ n$</p>\n<p>(3) 가중치 insight  </p>\n<p>가중치 W는 정답 target 만드는 입력 input 갯수만큼 필요하다. 그 데이터에 변화를 줘야하기 때문이다. 입력data 각각의 변화를 위해 행렬 곱 연산을 사용한다.</p>\n<p>=> $y = X*W + b$</p>\n<p>ex) input (1,2,3) -> target (0)<br>\ntarget 0 하나를 위해 input 3개 필요하니, 가중치도 3개 필요하다  </p>\n<p>입력 각각은 독립적이니까 $y = w1*x1 + w2*x2 + w3*x3 + b$  이렇게 표현할 수 있다.</p>\n<p>ex) input (1) -> target (2) : w 1개 필요</p>\n<p>cf. The Gradients of specified image is that, Y = a GramMatrix avg / a image</p>\n<ul>\n<li>gradient = 미분 활용</li>\n</ul>\n<hr>\n<h3 id=\"2-경사하강법-gradient-decent-algorithm\" style=\"position:relative;\">2. 경사하강법 gradient decent algorithm<a href=\"#2-%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95-gradient-decent-algorithm\" aria-label=\"2 경사하강법 gradient decent algorithm permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h3>\n<p>​(1) 손실 함수 Loss Function</p>\n<p>손실 함수의 계산</p>\n<p>손실함수 : $E(W,b) = (Σ [t_i-(W*x+b)]^2 )/ n$</p>\n<p>=> 하지만 오차 함수가 n차가 되면 함수를 그리기가 힘들어서 미분을 이용한다.<br>\n=> 기울기가 0이다 = 변화가 없다 = 최소값이다<br>\n=> gradient decent algorithm</p>\n<p>(2) Gradient Decent Algorithm</p>\n<ul>\n<li>경사하강법 : W에서의 직선의 기울기인 미분 값을 이용하여, 그 값이 작아지는 방향으로 진행하여 손실함수 최소값을 찾는 방법</li>\n</ul>\n<p>$W = W - (∂E(W,b) / ∂W) * a$<br>\n(a:학습율 Learning Rate)</p>\n<p>임의의 한 점에서 미분값을 구했을 때 양수이면 임의의 한 점이 감소시키는 방향으로, 아니면 그 반대인 증가시키는 방향으로 이동하게 된다.</p>\n<ul>\n<li>y=(x-a)^2+b 를 안 하는 이유? 2차함수는 이거는 입력변수 x가 하나지만, y=w1<em>x1\n+w2</em>x2+b 이런 형태이면 그래프가 3차원으로 나온다. 여기까지가 한계이다.<br>\n=> 함수를 시각적으로 그릴 수 없어서 미분 gradient을 활용하게 된다.</li>\n</ul>\n<p>(3) Learning Rate 학습율</p>\n<p>W 값의 감소 또는 증가 되는 비율 (0.1 미만)</p>\n<p>(4) Regression 에서의 [W, b] 계산 프로세스</p>\n<h4 id=\"-ml-기본-패턴\" style=\"position:relative;\">* ML 기본 패턴<a href=\"#-ml-%EA%B8%B0%EB%B3%B8-%ED%8C%A8%ED%84%B4\" aria-label=\" ml 기본 패턴 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h4>\n<ul>\n<li>데이터 정의 (x,t) 하는 부분  </li>\n<li>손실함수 정의해서 (임의의 W, b 값을 둬서) y 계산하는 부분 &#x26; E 계산하는 부분  </li>\n<li>for문 반복하여 W, b update하는 부분  </li>\n</ul>\n<p>동일한 입력에 대해 w, b에 따라 y가 달라진다. x에 대해 y (w*x+b)값 계산되고, e((y-t)^2) 값이 계산된다. 최소인지는 모르므로 이를 for문으로 반복해 가면서 비교하고, 이 안에서 w, b도 계속 바꾸는 식이다.</p>\n<p>(5) simple regression example</p>\n<ul>\n<li>학습데이터(Training Data) 준비</li>\n<li>임의의 직선 y = Wx + b 정의 (임의의 값으로 가중치 W, 바이어스 b 초기화)</li>\n<li>손실함수 E(W,b) 정의</li>\n<li>수치미분 numerical_derivative 및 predict 함수 정의</li>\n<li>학습율 (learning rate) 초기화 및 손실함수가 최소가 될 때까지 W, b 업데이트</li>\n<li>학습 결과 및 입력 값에 대한 미래 값 예측</li>\n</ul>\n<hr>\n<h3 id=\"3-문제-풀이-시간에-배운-점-예제3종\" style=\"position:relative;\">3. 문제 풀이 시간에 배운 점 (예제3종)<a href=\"#3-%EB%AC%B8%EC%A0%9C-%ED%92%80%EC%9D%B4-%EC%8B%9C%EA%B0%84%EC%97%90-%EB%B0%B0%EC%9A%B4-%EC%A0%90-%EC%98%88%EC%A0%9C3%EC%A2%85\" aria-label=\"3 문제 풀이 시간에 배운 점 예제3종 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h3>\n<p>예제1.  </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-python line-numbers\"><code class=\"language-python\"><span class=\"token operator\">//</span> batch 한 번에 표시\nxdata <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">1.0</span><span class=\"token punctuation\">,</span><span class=\"token number\">2.0</span><span class=\"token punctuation\">,</span><span class=\"token number\">3.0</span><span class=\"token punctuation\">,</span><span class=\"token number\">4.0</span><span class=\"token punctuation\">,</span><span class=\"token number\">5.0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span></span></pre></div>\n<p>cf. batch_size : 원래는 1개씩 넣어야하는데 5개 한번에 넣으니까 5</p>\n<p>cf. loss<em>func 함수임을 나타내고, loss</em>val 값을 나타내고 싶어서 굳이 나눠서 각각 정의했을 뿐이지 그냥 같은 것이다. 하나만 쓰기도 한다.</p>\n<p>학습 완료 후 (W,b setting 이후) predict</p>\n<p>=> 손실함수가 최소가 될 때까지 반복하며 W, b를 구함</p>\n<p>ex. learning rate 값을 1e-2에서 1e-1로 바꿨다 => 발산<br>\nex. learning rate 값을 1e-2에서 1e-6로 바꿨다 => 학습이 더딤<br>\nex. learning rate 값을 1e-2로 원복하고, epochs를 8001에서 2001로 바꿨다 => 학습이 덜 됨  </p>\n<p>cf. epochs : 학습을 얼마나 반복할 것인가</p>\n<hr>\n<h4 id=\"★-hyper-parameter\" style=\"position:relative;\">★ Hyper Parameter<a href=\"#%E2%98%85-hyper-parameter\" aria-label=\"★ hyper parameter permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h4>\n<ul>\n<li>\n<p>하이퍼 파라미터 = learning rate, epochs</p>\n<ul>\n<li>경험, 테스트로 알게될 값 => 값 찾는 게 어렵다.</li>\n<li>데이터에 민감하고, 환경에 민감하다. => 그러니 바꿔야 한다. 계속.  </li>\n<li>ML에서 어려운 건 하이퍼 파라미터 세팅이다</li>\n</ul>\n</li>\n</ul>\n<p>cf. 그래서 AutoML 연구가 활발하다고 한다.</p>\n<h4 id=\"-ml-tip\" style=\"position:relative;\">* ML Tip<a href=\"#-ml-tip\" aria-label=\" ml tip permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h4>\n<ol>\n<li>hyper parameter를 설정하고  </li>\n<li>손실함수가 줄어드는 거 확인한다. (절대값 아니고 trend 경향을 본다.)<br>\ncf. divergence 발산할 수도 있다 => 이 경우가 최악이며, 이 때는 학습율 learning rate 감소시켜야 한다.</li>\n</ol>\n<p><del>cf. trade off 이율 배반... 속도랑 정확도 같이 얻기 힘들다...<br>\n=> 회사나 팀에서 결정하는 편이긴 함.</del></p>\n<ol start=\"3\">\n<li>하이퍼파라미터를 찾는 나만의 노하우를 만들어 보자.</li>\n</ol>\n<p>cf. 손실함수가 발산할 때, 수렴하도록 하면 뭘 바꿔야할까 = learning rate !\ncf. ​정답을 만드는 입력 갯수 3개니까 가중치 3개 필요</p>\n<hr>\n<p>​예제2.<br>\n오차가 6이하로 안 내려가는 이유는, 일단 기본적으로 데이터가 부족해서이고, 어디가 최소인지 코드 돌리기 전엔 몰라서... 이기도 하고 그렇다.</p>\n<p>cf. 데이터는 다다익선, 백만 행 이상</p>\n<p>cf. local minimum / global minimum<br>\nlocal을 벗어나려면, 다양한 파라미터 값을 써봐야 한다.</p>\n<p>cf. np.random.seed(0) seed : 값을 주면 특정 seed에 대해 W 값은 똑같이 나온다. 코드 돌릴 때마다 난수 바뀌면 동일한 상태에서 테스트할 수 없어서... 그때 유용하다고.  </p>\n<p>예제3.<br>\nfor문 돌 때마다 Error 값 List에 append 하여 matplotlib로 그래프를 그려보자.</p>","tableOfContents":"<ul>\n<li>\n<p><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-7%EC%9D%BC%EC%B0%A8/#7%EC%9D%BC%EC%B0%A8\">7일차</a></p>\n<ul>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-7%EC%9D%BC%EC%B0%A8/#%EB%A9%94%EB%AA%A8\">메모</a></li>\n</ul>\n</li>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-7%EC%9D%BC%EC%B0%A8/#1-linear-regression\">1. Linear Regression</a></li>\n<li>\n<p><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-7%EC%9D%BC%EC%B0%A8/#2-%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95-gradient-decent-algorithm\">2. 경사하강법 gradient decent algorithm</a></p>\n<ul>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-7%EC%9D%BC%EC%B0%A8/#-ml-%EA%B8%B0%EB%B3%B8-%ED%8C%A8%ED%84%B4\">* ML 기본 패턴</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-7%EC%9D%BC%EC%B0%A8/#3-%EB%AC%B8%EC%A0%9C-%ED%92%80%EC%9D%B4-%EC%8B%9C%EA%B0%84%EC%97%90-%EB%B0%B0%EC%9A%B4-%EC%A0%90-%EC%98%88%EC%A0%9C3%EC%A2%85\">3. 문제 풀이 시간에 배운 점 (예제3종)</a></p>\n<ul>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-7%EC%9D%BC%EC%B0%A8/#%E2%98%85-hyper-parameter\">★ Hyper Parameter</a></li>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-7%EC%9D%BC%EC%B0%A8/#-ml-tip\">* ML Tip</a></li>\n</ul>\n</li>\n</ul>","frontmatter":{"title":"AI 이노베이션 스퀘어 12기 기본반 7일차 후기"}}},"pageContext":{"slug":"/ai-이노베이션-스퀘어-7일차/"}},"staticQueryHashes":[]}