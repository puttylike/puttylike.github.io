{"componentChunkName":"component---src-templates-blog-post-js","path":"/ai-이노베이션-스퀘어-12일차/","result":{"data":{"markdownRemark":{"html":"<h3 id=\"12일차\" style=\"position:relative;\">12일차<a href=\"#12%EC%9D%BC%EC%B0%A8\" aria-label=\"12일차 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h3>\n<ul>\n<li>mnist, 말로만 듣고 코딩을 해본 건 처음이라 설렜다. 강의 들은 건 많은데 코딩에 쓴 시간이 적었던 게 이전까지 발전이 없었던 이유가 아니었나 싶기도 하다. 오늘 부로 ML, DL 기본 단계를 마친다.</li>\n<li>다음 주 부터는 중급 과정인 오차역전파에 입문한다. 작년에 오차역전파 할 때 자괴감 들어서 내가 수학과가 맞나 싶었는데, 이렇게 차근차근 배우지 않아서 코딩과 수학이 따로 논 게 아니었나 싶기도 하고... 기분이 오묘하다. ㅎㅎ</li>\n<li>빅데이터 분석기사, 일단 공부 시작...! 공부할 건 너무 많다. 비전공자는 이런 건가...?</li>\n</ul>\n<h4 id=\"메모\" style=\"position:relative;\">메모<a href=\"#%EB%A9%94%EB%AA%A8\" aria-label=\"메모 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h4>\n<ul>\n<li>입력 데이터 정규화 &#x26; technic</li>\n<li>이미지 Xavier/he by 정규분포  </li>\n<li>img는 data preprocess = 정규화</li>\n<li>one-hot encoding  </li>\n<li>손실함수 그려봐야 한다, 중간에 outlier 특이점들 drop할지 들고갈 지 회사의 의사결정 process가 있어야 한다</li>\n<li>미션크리티컬 금융/의료는 안 된다 (돈/생명)</li>\n</ul>\n<hr>\n<h3 id=\"문제-풀이-시간에-배운-점-어제에-이어서\" style=\"position:relative;\">문제 풀이 시간에 배운 점 (어제에 이어서)<a href=\"#%EB%AC%B8%EC%A0%9C-%ED%92%80%EC%9D%B4-%EC%8B%9C%EA%B0%84%EC%97%90-%EB%B0%B0%EC%9A%B4-%EC%A0%90-%EC%96%B4%EC%A0%9C%EC%97%90-%EC%9D%B4%EC%96%B4%EC%84%9C\" aria-label=\"문제 풀이 시간에 배운 점 어제에 이어서 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h3>\n<p>​\n변곡점 1개인 sigmoid(기존에 쓰던 activation function)의 한계로,<br>\nmnist 출력층의 정답을 표시하는 방식으로 one-hot encoding 방식을 쓰게 된다.</p>\n<p>cf. Keras - to_categorical( , )​</p>\n<p>np.argmax 사용하여 여러 값 중 max 값에 대한 인덱스를 찾고,<br>\n그 인덱스 값을 찾은 정답으로 간주하는 식이다.\n=</p>\n<h4 id=\"예제1\" style=\"position:relative;\">예제1<a href=\"#%EC%98%88%EC%A0%9C1\" aria-label=\"예제1 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h4>\n<p>diabetes 정답 data를 one-hot encoding으로 나타내기.<br>\n=> 기존 코드에서 정답 데이터 부분 바꾸는 게 필요하다</p>\n<p>y도, target_data도 형태는 (2, ) 로 동일하게 하였기 때문에,<br>\n기존의 ​cross entropy 코드를 그대로 사용할 수 있다.<br>\n(... 이런 느낌 : np.array([0,1])*np.array([1,2]))</p>\n<p>cf. legacy code<br>\n사실, class는 data type이라 함부로 바꾸면 안 된다.<br>\n밖에서 y 형태를 바꿨으나 class 안에 있는 cross-entropy 불변 해야 하므로,<br>\nloss func 쓰기 위해 밖에서 target 형태를 변경했다.</p>\n<p>\"flexibility ↑ complexity ↑ = trade-off\"</p>\n<hr>\n<h4 id=\"-정규화\" style=\"position:relative;\">* 정규화<a href=\"#-%EC%A0%95%EA%B7%9C%ED%99%94\" aria-label=\" 정규화 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h4>\n<p>입력 데이터에 대하여 0 ~ 1 사이 값으로 바꾸는 작업이 필요하다.</p>\n<ul>\n<li>(data-최소) / (최대-최소) : 이런 식으로 계산하면 된다.</li>\n</ul>\n<p>$0 < (data - min)/(max - min) < 1$<br>\n$(only_if max - min = real num)$</p>\n<ul>\n<li>정규화를 해야 하는 이유?</li>\n</ul>\n<p>일단 loss가 커진다,<br>\nloss가 일정하게 감소하지 않고 출렁거린다. (fluctuation)<br>\n입력을 0~1 사이 소숫점으로 만들면, loss가 어느 범위 안에 있게 된다.</p>\n<p>=> 다시 말해, loss를 줄이기 위해 w가 변하는데, loss가 크면 w가 많이 변해야 한다.<br>\n기존의 data에 더 많은 영향을 미치게 돼 loss가 계속 출렁이게 된다.</p>\n<p>... 가둬 둬야 loss가 일정하게 더 빨리 줄여나갈 수 있다.</p>\n<ul>\n<li>normalization 통계학적 의미?</li>\n<li>0은 값 없는 취급 받게 되니까, 0.01~1 사이로 만듦 => technic</li>\n</ul>\n<hr>\n<h4 id=\"예제3\" style=\"position:relative;\">예제3<a href=\"#%EC%98%88%EC%A0%9C3\" aria-label=\"예제3 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h4>\n<p>이미지 data는 학습 전 정규화 one-hot encoding (data process) 필요하다.</p>\n<ul>\n<li>이미지에서 기존에 쓰던 가중치 W 초기화 np.random.rand(...) 방식은 유효할까?</li>\n<li>학습 후, 실제 오차 검증이 필요하긴 하다<br>\n원래 실제 오차 data 7->1 이런 경우, 오류 확인 작업이 있어야 맞음 = ml debugging</li>\n</ul>\n<h4 id=\"-가중치-with-randn-by-xavierhe\" style=\"position:relative;\">* 가중치 with randn by Xavier/He<a href=\"#-%EA%B0%80%EC%A4%91%EC%B9%98-with-randn-by-xavierhe\" aria-label=\" 가중치 with randn by xavierhe permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h4>\n<ul>\n<li>Xavier와 He가 고안한 방식으로, randn을 활용해<br>\nstd 1 &#x26; avg 0 정규분포에서 -1 ~ 1 사이 값으로 W 값을 추출하게 바꾸면 성능이 더 좋다.​</li>\n</ul>\n<p>이미지는 0~1 사이 w 많은데 너무 random 하다 이렇게 중구난방으로 뽑으면 학습이 어렵기 때문이다.</p>\n<p>표준편차 내에서 대칭이 되게 뽑는다.</p>\n<p>​</p>\n<h4 id=\"-image-data-insight\" style=\"position:relative;\">* image data insight<a href=\"#-image-data-insight\" aria-label=\" image data insight permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h4>\n<ul>\n<li>image : adjacent pixel is nearly same as each other</li>\n</ul>\n<p>255|0 => edge detection<br>\n이미지에서는 두 픽셀의 편차가 크면 경계 edge로 간주할 수 있다.</p>\n<p>​숫자 근처엔 비슷한 숫자들이 있으니 대칭으로 뽑아 상쇄시킨다.</p>\n<ul>\n<li>img data는 픽셀로 나타내 입력 노드 수가 많다.  </li>\n<li>가중치가 많은데 random으로 뽑으면 값이 너무 커진다.  </li>\n<li>img 값들은 근처 값끼리 data가 같으니 대칭으로 뽑아 상쇄시켜 loss를 일정 범위 이내로 축소시킬 수 있다.  </li>\n</ul>\n<p>​cf.</p>\n<ul>\n<li>tf.random.normal = np.random.randn(..) = normal distribution  </li>\n<li>tf,random.uniform = np.random.rand(..)</li>\n</ul>\n<p>=> 앞으로 이미지는 randn을 쓸 것.</p>\n<ul>\n<li>평균의 함정 => 중간값과 평균의 차이 check / standard deviation 표준편차 std</li>\n</ul>\n<p>이상적으로 smooth하게 떨어지는 loss func은 없다.<br>\n거의 대부분이 들쭉날쭉 하면서 내려간다.</p>\n<p>​손실이 크다는 건 정답과 계산 값이 크다는 것이다. 정답은 고정된 값이니, 계산 값이 들쭉날쭉 하다는 것이다.</p>\n<p>w 변화로 y가 변한다. updated W는 다음 data에 영향을 미친다.</p>\n<p>​\ncf. 상관계수</p>\n<p>​* 확률의 함정 = 가능성, 학률 1/2이란 모집단 무한대일 때 1/2 근사한다는 의미.  </p>\n<p>vague 희미한 이미지 끊긴 글자 이런 거로, loss 가 튀곤 한다.<br>\n이미지 근처에 숫자가 몰려있을 가능성이 높다고 볼 수 있는 것도 확률적이다.\n​\n=> ambiguous data처럼 loss 튈 때 어느 data인지 분석해야 한다.<br>\n= 손실함수 값 계속 저장해서, loss 값 max의 index를 찾아 (argmax) show한다.</p>\n<p>튀는 값 때문에 w가 많이 변하게 될 수도 있다.<br>\n그렇다고 애매한 건 drop 해야 할까? => 회사라면 decision making이 필요하다.</p>\n<p>cf. mission critical</p>\n<p>​하지만, 금융/의료 = 데이터를 쉽게 못 없앤다 (돈, 생명)<br>\n= 100%라는 게 없는 분야이고, 그래서 특이점 있을 수 밖에 없다.</p>\n<p>loss 급등 하다 돌아오면, 속도가 오래 걸리니까<br>\n<strong>미션 크리티컬 아니면</strong> drop 고려가 괜찮다는 decision making 필요하다.</p>\n<p>​ex. 생소한 주가 급락 = loss 급증 => 그럼에도 데이터를 버릴 순 없다</p>\n<p>ex. 천재들의 머니게임 중 ltcm은 나라가 망할 10^-14의 확률 outlier로 망했다.</p>\n<hr>\n<p>프로그램 잘 짜도 미분 때문에 시간이 너무 오래 걸려서<br>\n앞으로는 오차역전파를 쓰게 될 예정이다.</p>\n<p>...미분 대신 오차역전파 쓰니까 20시간이 26초 ㅋㅎ 정확도는 비슷한데 ;;</p>\n<p>,,,trade off 오차역전파로 손,머리 과부화 ↑ / 시간 ↓ ㅎㅎ...</p>","tableOfContents":"<ul>\n<li>\n<p><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-12%EC%9D%BC%EC%B0%A8/#12%EC%9D%BC%EC%B0%A8\">12일차</a></p>\n<ul>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-12%EC%9D%BC%EC%B0%A8/#%EB%A9%94%EB%AA%A8\">메모</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-12%EC%9D%BC%EC%B0%A8/#%EB%AC%B8%EC%A0%9C-%ED%92%80%EC%9D%B4-%EC%8B%9C%EA%B0%84%EC%97%90-%EB%B0%B0%EC%9A%B4-%EC%A0%90-%EC%96%B4%EC%A0%9C%EC%97%90-%EC%9D%B4%EC%96%B4%EC%84%9C\">문제 풀이 시간에 배운 점 (어제에 이어서)</a></p>\n<ul>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-12%EC%9D%BC%EC%B0%A8/#%EC%98%88%EC%A0%9C1\">예제1</a></li>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-12%EC%9D%BC%EC%B0%A8/#-%EC%A0%95%EA%B7%9C%ED%99%94\">* 정규화</a></li>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-12%EC%9D%BC%EC%B0%A8/#%EC%98%88%EC%A0%9C3\">예제3</a></li>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-12%EC%9D%BC%EC%B0%A8/#-%EA%B0%80%EC%A4%91%EC%B9%98-with-randn-by-xavierhe\">* 가중치 with randn by Xavier/He</a></li>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-12%EC%9D%BC%EC%B0%A8/#-image-data-insight\">* image data insight</a></li>\n</ul>\n</li>\n</ul>","frontmatter":{"title":"AI 이노베이션 스퀘어 12기 기본반 12일차 후기"}}},"pageContext":{"slug":"/ai-이노베이션-스퀘어-12일차/"}},"staticQueryHashes":["3159585216"]}