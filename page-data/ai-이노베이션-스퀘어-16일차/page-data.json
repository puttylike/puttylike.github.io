{"componentChunkName":"component---src-templates-blog-post-js","path":"/ai-이노베이션-스퀘어-16일차/","result":{"data":{"markdownRemark":{"html":"<h3 id=\"16일차\" style=\"position:relative;\">16일차<a href=\"#16%EC%9D%BC%EC%B0%A8\" aria-label=\"16일차 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h3>\n<ul>\n<li>CNN에 진입했다. 예전에는 param 갯수 세는 거 이해 못 했는데, 이번엔 이해... 잘 하고 있는 듯. 코드만 익히면 될 거 같아 내심 기분이 좋다. fully connected 부터 다음주에 배운다.</li>\n<li>오차역전파를 다 흡수 못 했는지 순간적으로 ndmin=2 까먹었다. 역시 복습이 생명... ㅠㅠ</li>\n</ul>\n<h4 id=\"메모\" style=\"position:relative;\">메모<a href=\"#%EB%A9%94%EB%AA%A8\" aria-label=\"메모 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h4>\n<ul>\n<li>Mount ?  </li>\n<li>Lazy evaluation ?  </li>\n</ul>\n<hr>\n<h4 id=\"은닉층\" style=\"position:relative;\">은닉층<a href=\"#%EC%9D%80%EB%8B%89%EC%B8%B5\" aria-label=\"은닉층 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h4>\n<p>​\ncf. CNN을 tf 2. 버전 대에서 하면 코드가 함축적이라 내부를 알기 힘들어 이 수업에서는 1. 버전 대에서 수행한다.\n​</p>\n<p>(i) NN => CNN, RNN (LSTM, GRU, ...)<br>\n=> 은닉층 modify : 딥러닝 아키텍처가 바뀐다는 건 은닉층 구조가 바뀐다는 의미다.<br>\n(ii) CNN (이미지 처리) => 입력층 shape<br>\n현재까지 28*28 image를 행렬을 벡터로 변경했다. CNN에서는 vector 아닌 tensor로 해줘야 한다.</p>\n<p>★ 고급 아키텍처 : 은닉층 hidden layer 변경 => hidden layer = major component<br>\n특히 image 처리를 위해 정확도를 높히려면, ★ (x,y, color) tensor 로 써야 한다.<br>\n=> 실제 처리하는 건 은닉층이다만, 은닉층에서 잘 처리하기 위해서는 입력층이 바뀌어야한다.</p>\n<hr>\n<h4 id=\"convolution\" style=\"position:relative;\">Convolution<a href=\"#convolution\" aria-label=\"convolution permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h4>\n<p>$f(t) * g(t) = ∫f(τ)g(t-τ)dτ$<br>\n적분 변수와 결과로 나온 변수가 다르다.<br>\n=> t에 대한 함수이다. = t에 대해 변한다.</p>\n<p>f 원본 data에 g 만큼의 변화를 준다고 볼 수 있는데, 적분은 연속적인 더하기 (sigma는 불연속) 이므로 <strong>원본 변화의 평균</strong>을 의미한다고 볼 수 있다.</p>\n<p>cf. 행렬곱 : XㆍW 원본 data에 평균적으로 변화를 줄 때 얼마나 변하는지 알 때</p>\n<p>★ 여기서는 t 변화로 다양한 변화를 줄 수 있다. (행렬곱은 정적이다.)</p>\n<p>insight => \"data variation , average , time shift\"</p>\n<p>Conv는 ​<strong>시간에 따라 원본에 대한 평균적인 변화를 알고 싶을 때 사용한다.</strong></p>\n<p>cf. 원본을 평균적으로 변화시키고 싶을 때는 행렬 곱,<br>\n입력의 미세한 변화에 얼마나 출력이 반응하나 볼 때는 미분 !</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Convolution\">예시</a><br>\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b9/Convolution_of_spiky_function_with_box2.gif\"><br>\n=> 원본에 변화를 주는데, 원본의 특징 feature을 뽑아 내면서 변화를 준다<br>\n(원본 f가 오른쪽으로 감소하면, conv 결과도 결국 오른쪽으로 감소하는 특징을 띈다.)</p>\n<hr>\n<h3 id=\"cnn\" style=\"position:relative;\">CNN<a href=\"#cnn\" aria-label=\"cnn permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h3>\n<ul>\n<li>CNN 아키텍처 : 은닉층 1개 이상의 conv + pool + 완전 연결층 을 갖고 있다.</li>\n</ul>\n<p>(1) 컨볼루션층 개요</p>\n<ol>\n<li>pooling - max, min, avg,,,<br>\n(max pooling) 주어진 data size에서 가장 큰 값을 추출하는 연산을 수행하는데, 데이터 특징을 압축하여 데이터의 연산량을 줄여주는 역할을 한다. 보통 max pooling을 가장 많이 쓴다.</li>\n<li>\n<p>conv (convolution)<br>\n입력과 filter의 convolution 연산을 통해 <strong>입력 data의 feature을 추출</strong>한다.  </p>\n<ul>\n<li>f(τ) : input  </li>\n<li>g(t-τ) : filter</li>\n</ul>\n</li>\n</ol>\n<p>컴퓨터에서 다루는 값은 이산적이다.</p>\n<p>​filter (kernel)는 가중치 W 집합체이다.</p>\n<ol start=\"3\">\n<li>\n<p>컨볼루션 (convolution) 연산 * – 특징 추출 (특징 맵, feature map)</p>\n<ul>\n<li>stride : 필터의 이동 간격, 시간적으로 이동한다<br>\nex) stride = 1 => 1칸 씩 이동</li>\n</ul>\n</li>\n</ol>\n<p>★ feature map : XㆍW + b => filter 1개 당 feature map 1개 나온다. ?<br>\nbias는 filter 당 1개이다.</p>\n<p>Y = XㆍW + b<br>\nY &#x3C;-> T => loss</p>\n<p>ex. X=10, T=100 : W = 6이면 10씩 올린다 치면, 5번 수행해야 오차가 최소가 된다.<br>\n=> b가 +10 있는 게 더 적은 횟수로 오차가 최소가 될 수 있다.<br>\n=> <strong>bias : 원래 계산한 값의 가속 또는 감속을 해준다</strong><br>\n= W : data variation\n= b : adjustment</p>\n<p>=> bias : 어떤 특징을 빨리 찾아가게 보정<br>\nfeature map : 원본 data 특징을 간직</p>\n<p>은닉층 갯수, 노드나 conv 수, 완전 연결층 수 비롯해서 conv filter size, stride 수치 모두 hyper paremeter이다.</p>\n<p>​Convolution의 핵심은 filter이다.<br>\nex. 필터 (가중치 집합체) size가 3x3 이면, 가중치는 9개다.<br>\nex. ​4x4 filter 가 4개이면 w는 64개이고, bias는 filter 당 1개니까 4개이다.</p>\n<p><strong>​★ filter size => weight / a bias per filter</strong></p>\n<p>ex. kernel 3x3 / filters 50 => w 갯수 450 , b 갯수 50</p>\n<p>ex. ​4x4 입력 data > conv 2x2 > relu > pooling > 1x1<br>\n=> 1x1가 되니 또 conv를 할 수가 없다.\n=> 이처럼 conv 시 shape이 줄어드는 문제를 해결하는 게 padding이다.</p>\n<ol start=\"4\">\n<li>\n<p>padding<br>\n원본 크기 줄어드는 걸 방지한다.\n원본에 padding하여 약간의 오차 발생하는 건, 어쩔 수 없다.</p>\n<ul>\n<li>padding = 1 : 껍데기 1</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h4 id=\"예제0\" style=\"position:relative;\">예제0<a href=\"#%EC%98%88%EC%A0%9C0\" aria-label=\"예제0 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h4>\n<p><a href=\"https://en.wikipedia.org/wiki/Kernel_(image_processing)\">Filter Sample</a></p>\n<p>​rgb 가 0~255 사이 값이니까, conv 결과 값이 255 넘으면 255로 세팅해 줘야 한다.</p>\n<p><strong>​★ 딥러닝으로 vertical 성분을 찾으려면 vertical loss func 값이 min 되는 w를 찾으면 된다.</strong></p>\n<p>예전에는 수학자들이 계산을 해서 filter 값을 구했지만, 지금은 정답 vertical 에 대한 loss min 되는 W 찾으면 그게 filter가 된다.</p>\n<hr>","tableOfContents":"<ul>\n<li>\n<p><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-16%EC%9D%BC%EC%B0%A8/#16%EC%9D%BC%EC%B0%A8\">16일차</a></p>\n<ul>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-16%EC%9D%BC%EC%B0%A8/#%EB%A9%94%EB%AA%A8\">메모</a></li>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-16%EC%9D%BC%EC%B0%A8/#%EC%9D%80%EB%8B%89%EC%B8%B5\">은닉층</a></li>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-16%EC%9D%BC%EC%B0%A8/#convolution\">Convolution</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-16%EC%9D%BC%EC%B0%A8/#cnn\">CNN</a></p>\n<ul>\n<li><a href=\"/ai-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%8A%A4%ED%80%98%EC%96%B4-16%EC%9D%BC%EC%B0%A8/#%EC%98%88%EC%A0%9C0\">예제0</a></li>\n</ul>\n</li>\n</ul>","frontmatter":{"title":"AI 이노베이션 스퀘어 13기 기본반 16일차 후기"}}},"pageContext":{"slug":"/ai-이노베이션-스퀘어-16일차/"}},"staticQueryHashes":["3159585216"]}